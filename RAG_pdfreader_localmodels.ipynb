{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xóa thư mục chromadb cũ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# --- 1. Xoá thư mục chromadb cũ nếu tồn tại ---\n",
    "if os.path.isdir(\"chromadb\"):\n",
    "    print(\"Đang xóa thư mục chromadb cũ...\")\n",
    "    shutil.rmtree(\"chromadb\")\n",
    "\n",
    "# --- 3. Kiểm tra và cài đặt các thư viện cần thiết ---\n",
    "def ensure_package(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Thiếu thư viện '{pkg_name}', đang cài đặt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        __import__(import_name or pkg_name)\n",
    "\n",
    "pkgs = [\n",
    "    (\"chromadb\", None),\n",
    "    (\"langchain\", None),\n",
    "    (\"ollama\", None),\n",
    "    (\"tiktoken\", None),\n",
    "    (\"PyPDF2\", None)\n",
    "]\n",
    "\n",
    "for pkg, imp in pkgs:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Đọc PDF với decoder UTF-8 để tránh lỗi tuple ---\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "pdf_path = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"  # Thay đường dẫn tới file PDF của bạn\n",
    "reader = PdfReader(pdf_path)\n",
    "docs = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    raw_text = page.extract_text() or \"\"\n",
    "    # Nếu raw_text là bytes, decode bằng utf-8 và bỏ ký tự không hợp lệ\n",
    "    if isinstance(raw_text, (bytes, bytearray)):\n",
    "        text = raw_text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = raw_text\n",
    "    docs.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": f\"{os.path.basename(pdf_path)}_page_{i+1}\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng embedding model local\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b79c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Tạo hoặc tải lại vector database local với fallback khi embed thất bại ---\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"🎉 VectorDB khởi tạo thành công với model embedding hiện tại.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"[Warning] Embed thất bại: {e}\")\n",
    "    print(\"Chuyển sang model embedding thay thế: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    # Cài và dùng MiniLM embedding local thay thế\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    fallback_embed = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=fallback_embed,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    embed_model = fallback_embed  # cập nhật cho phần truy vấn sau\n",
    "    print(\"🎉 VectorDB đã khởi tạo lại với MiniLM embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : ✓\n",
      "Model     : gemini-2.5-pro\n",
      "MaxTokens : 1048\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # đọc các biến GEMINI_API_KEY, GEMINI_MODEL,…\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\") # ví dụ: \"gemini-2.5-pro\"\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"Bạn là một trợ lý hữu ích. Hãy trả lời đầy đủ và chi tiết. Đừng lặp lại câu trả lời nếu không cần thiết.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Kiểm tra\n",
    "print(\"API Key   :\", \"✓\" if API_KEY else \"✗\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0107282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Định nghĩa và test với debug\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from IPython.display import clear_output\n",
    "# Patch để chạy asyncio.run()/await trong Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\")       # \"gemini-2.5-pro\"\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "BASE_URL = f\"https://generativelanguage.googleapis.com/v1/models/{MODEL_ID}:generateContent\"\n",
    "\n",
    "async def generate_content(prompt: str) -> dict:\n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\n",
    "            \"maxOutputTokens\": MAX_TOKENS,\n",
    "            \"temperature\": TEMP\n",
    "        }\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(BASE_URL, params={\"key\": API_KEY}, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "async def main(prompt: str):\n",
    "    result = await generate_content(prompt)\n",
    "    \n",
    "    # 1) In raw response để debug\n",
    "    print(\"🔍 Raw response:\")\n",
    "    import pprint; pprint.pprint(result)\n",
    "    \n",
    "    # 2) Show finishReason nếu có\n",
    "    fr = result.get(\"candidates\", [{}])[0].get(\"finishReason\")\n",
    "    print(f\"\\n⚙️ finishReason: {fr}\")\n",
    "\n",
    "    # 3) Extract text nếu có\n",
    "    cands = result.get(\"candidates\", [])\n",
    "    if not cands:\n",
    "        print(\"❌ No candidates returned.\")\n",
    "        return\n",
    "    content_obj = cands[0].get(\"content\", {})\n",
    "    parts = content_obj.get(\"parts\") or []\n",
    "    # Ghép text và loại bỏ newline ngay tại đây:\n",
    "    text = \"\".join(p.get(\"text\", \"\") for p in parts)\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    if text:\n",
    "        print(\"\\n🎉 Gemini trả lời đầy đủ:\\n\")\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"\\n⚠️ Không tìm thấy text trong parts.\")\n",
    "        print(\" Nếu finishReason là MAX_TOKENS, thử tăng GEMINI_MAX_TOKENS lên 512 hoặc 1024 và chạy lại.\")\n",
    "    \n",
    "    \n",
    "    # display(Markdown(clean_text))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e33bd0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chào bạn, Để tóm tắt chương 1 của tài liệu PDF cho bạn, tôi cần có nội dung của chương đó. Vì lý do bảo mật và kỹ thuật, tôi không thể truy cập trực tiếp vào các tệp tin trên máy tính của bạn. Bạn có thể cung cấp nội dung của chương 1 bằng một trong những cách sau: **Cách 1: Sao chép và Dán (Copy & Paste)** Đây là cách đơn giản và nhanh nhất nếu chương 1 không quá dài. Bạn chỉ cần: 1. Mở tệp PDF. 2. Bôi đen và sao chép (copy) toàn bộ văn bản của chương 1. 3. Dán (paste) nội dung đó vào đây. **Cách 2: Gửi liên kết (Link)** Nếu tài liệu PDF của bạn được đăng tải công khai trên mạng, bạn hãy gửi cho tôi liên kết đến tệp đó. *(Lưu ý: Tôi chỉ có thể truy cập các liên kết công khai, không yêu cầu đăng nhập hay mật khẩu).* **Cách 3: Mô tả các ý chính** Nếu bạn không thể sao chép nội dung, bạn có thể tự đọc và liệt kê ra các đề mục chính, các ý tưởng cốt lõi hoặc những đoạn quan trọng nhất trong chương 1. Dựa trên đó, tôi sẽ giúp bạn viết thành một bản tóm tắt hoàn chỉnh. --- Để bản tóm tắt được chính xác và hữu ích nhất, bạn có thể cho tôi biết thêm: * **Tên tài liệu và chủ đề chính là gì?** (Ví dụ: \"Chương 1: Giới thiệu về Trí tuệ nhân tạo\" trong sách \"AI cơ bản\"). * **Bạn cần bản tóm tắt này cho mục đích gì?** (Ví dụ: để ôn thi, làm báo cáo, hay chỉ để nắm ý chính). Sau khi bạn cung cấp nội dung, tôi sẽ tóm tắt lại cho bạn ngay lập tức\n"
     ]
    }
   ],
   "source": [
    "# Chạy test\n",
    "test_prompt = \"Tóm tắt chương 1 của tài liệu PDF cho tôi.\"\n",
    "asyncio.run(main(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Định nghĩa wrapper GeminiLLM dùng call_gemini\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    api_key:    str\n",
    "    max_output_tokens: int = 1000\n",
    "    temperature:       float = 0.6\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Gọi hàm call_gemini đã định nghĩa ở Cell 3\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Khởi tạo pipeline RAG với GeminiLLM và vectordb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Khởi tạo GeminiLLM\n",
    "gemini_llm = GeminiLLM(\n",
    "    model_name=MODEL_ID,            # từ Cell 2\n",
    "    api_key=API_KEY,                # từ Cell 2\n",
    "    max_output_tokens=MAX_TOKENS,   # từ Cell 2\n",
    "    temperature=TEMP                # từ Cell 2\n",
    ")\n",
    "\n",
    "# 2) Tạo RetrievalQA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 3) Định nghĩa hàm ask để hỏi và in kết quả + nguồn\n",
    "def ask(question: str):\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"=== ANSWER ===\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata.get('source', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7081e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"describe details as much as you can about Smell and Heuristic General.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
