{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang x√≥a th∆∞ m·ª•c chromadb c≈©...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# --- 1. Xo√° th∆∞ m·ª•c chromadb c≈© n·∫øu t·ªìn t·∫°i ---\n",
    "if os.path.isdir(\"chromadb\"):\n",
    "    print(\"ƒêang x√≥a th∆∞ m·ª•c chromadb c≈©...\")\n",
    "    shutil.rmtree(\"chromadb\")\n",
    "\n",
    "# --- 3. Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ---\n",
    "def ensure_package(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Thi·∫øu th∆∞ vi·ªán '{pkg_name}', ƒëang c√†i ƒë·∫∑t...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        __import__(import_name or pkg_name)\n",
    "\n",
    "pkgs = [\n",
    "    (\"chromadb\", None),\n",
    "    (\"langchain\", None),\n",
    "    (\"ollama\", None),\n",
    "    (\"tiktoken\", None),\n",
    "    (\"PyPDF2\", None)\n",
    "]\n",
    "\n",
    "for pkg, imp in pkgs:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ƒê·ªçc PDF v·ªõi decoder UTF-8 ƒë·ªÉ tr√°nh l·ªói tuple ---\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "pdf_path = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"  # Thay ƒë∆∞·ªùng d·∫´n t·ªõi file PDF c·ªßa b·∫°n\n",
    "reader = PdfReader(pdf_path)\n",
    "docs = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    raw_text = page.extract_text() or \"\"\n",
    "    # N·∫øu raw_text l√† bytes, decode b·∫±ng utf-8 v√† b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá\n",
    "    if isinstance(raw_text, (bytes, bytearray)):\n",
    "        text = raw_text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = raw_text\n",
    "    docs.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": f\"{os.path.basename(pdf_path)}_page_{i+1}\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2051d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79dc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klein\\AppData\\Local\\Temp\\ipykernel_40136\\1977429318.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# S·ª≠ d·ª•ng embedding model local\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b79c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ VectorDB kh·ªüi t·∫°o th√†nh c√¥ng v·ªõi model embedding hi·ªán t·∫°i.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klein\\AppData\\Local\\Temp\\ipykernel_40136\\4150500916.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# --- 7. T·∫°o ho·∫∑c t·∫£i l·∫°i vector database local v·ªõi fallback khi embed th·∫•t b·∫°i ---\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"üéâ VectorDB kh·ªüi t·∫°o th√†nh c√¥ng v·ªõi model embedding hi·ªán t·∫°i.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"[Warning] Embed th·∫•t b·∫°i: {e}\")\n",
    "    print(\"Chuy·ªÉn sang model embedding thay th·∫ø: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    # C√†i v√† d√πng MiniLM embedding local thay th·∫ø\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    fallback_embed = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=fallback_embed,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    embed_model = fallback_embed  # c·∫≠p nh·∫≠t cho ph·∫ßn truy v·∫•n sau\n",
    "    print(\"üéâ VectorDB ƒë√£ kh·ªüi t·∫°o l·∫°i v·ªõi MiniLM embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : ‚úì\n",
      "Model     : gemini-2.0\n",
      "MaxTokens : 1048\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # ƒë·ªçc c√°c bi·∫øn GEMINI_API_KEY, GEMINI_MODEL,‚Ä¶\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\") # v√≠ d·ª•: \"gemini-1.5-pro\"\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch. H√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt. ƒê·ª´ng l·∫∑p l·∫°i c√¢u tr·∫£ l·ªùi n·∫øu kh√¥ng c·∫ßn thi·∫øt.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Ki·ªÉm tra\n",
    "print(\"API Key   :\", \"‚úì\" if API_KEY else \"‚úó\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5521df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent?key=AIzaSyBWhaJdRNdbK_wbVs7iddmpN0z3nwMhcNc\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-2.5-pro:generateContent?key=AIzaSyBWhaJdRNdbK_wbVs7iddmpN0z3nwMhcNc\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBWhaJdRNdbK_wbVs7iddmpN0z3nwMhcNc\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=AIzaSyBWhaJdRNdbK_wbVs7iddmpN0z3nwMhcNc\n",
      "\n",
      "=== gemini-1.5-pro (HTTP 429) ===\n",
      "{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\\nPlease retry in 45.019685852s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-1.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-1.5-pro', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '45s'}]}}\n",
      "\n",
      "=== gemini-2.5-pro (HTTP 503) ===\n",
      "{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "=== gemini-2.0-flash (HTTP 200) ===\n",
      "{'candidates': [{'content': {'parts': [{'text': 'I am doing well, thank you for asking! How are you today?\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.02326473593711853}], 'usageMetadata': {'promptTokenCount': 6, 'candidatesTokenCount': 16, 'totalTokenCount': 22, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 6}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 16}]}, 'modelVersion': 'gemini-2.0-flash', 'responseId': 'p4rSaNrYLYaLqtsP_e_g6A8'}\n",
      "\n",
      "=== gemini-1.5-flash (HTTP 200) ===\n",
      "{'candidates': [{'content': {'parts': [{'text': 'I am doing well, thank you for asking!  How are you today?\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.023443025701186237}], 'usageMetadata': {'promptTokenCount': 6, 'candidatesTokenCount': 17, 'totalTokenCount': 23, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 6}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 17}]}, 'modelVersion': 'gemini-1.5-flash', 'responseId': 'qIrSaLDfM5_mqtsPocmK2Q8'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Load your API key\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Please set GEMINI_API_KEY in your environment (.env)\")\n",
    "\n",
    "# Generation settings\n",
    "MAX_TOKENS = 1024\n",
    "TEMP = 0.7\n",
    "\n",
    "# Rate-limit state\n",
    "last_call_time = 0.0\n",
    "\n",
    "# Danh s√°ch model ∆∞u ti√™n (unchanged)\n",
    "MODEL_FALLBACKS = [\n",
    "    \"gemini-1.5-pro\",\n",
    "    \"gemini-2.5-pro\",\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-1.5-flash\"\n",
    "]\n",
    "\n",
    "def call_gemini(prompt: str) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Calls each model once and returns a dict:\n",
    "      {\n",
    "        \"model-name\": {\n",
    "          \"status_code\": int,\n",
    "          \"response\": dict or str\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    global last_call_time\n",
    "    results: dict[str, dict] = {}\n",
    "\n",
    "    for model in MODEL_FALLBACKS:\n",
    "        # Enforce 0.5s between calls\n",
    "        now = time.time()\n",
    "        wait = 0.5 - (now - last_call_time)\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "        last_call_time = time.time()\n",
    "\n",
    "        # Determine endpoint version\n",
    "        version = \"v1\" if model.startswith((\"gemini-2.5\", \"gemini-1.5\")) else \"v1beta\"\n",
    "        \n",
    "        url = (\n",
    "            f\"https://generativelanguage.googleapis.com/\"\n",
    "            f\"{version}/models/{model}:generateContent?key={API_KEY}\"\n",
    "        )\n",
    "        print(f\"[DEBUG] G·ªçi URL: {url}\")\n",
    "\n",
    "        # Make the single request\n",
    "        resp = requests.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "                \"generationConfig\": {\n",
    "                    \"maxOutputTokens\": MAX_TOKENS,\n",
    "                    \"temperature\": TEMP\n",
    "                }\n",
    "            },\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        # Try to parse JSON; fallback to plain text\n",
    "        try:\n",
    "            body = resp.json()\n",
    "        except ValueError:\n",
    "            body = resp.text\n",
    "\n",
    "        results[model] = {\n",
    "            \"status_code\": resp.status_code,\n",
    "            \"response\": body\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Hello, how are you?\"\n",
    "    all_outputs = call_gemini(prompt)\n",
    "    for model, info in all_outputs.items():\n",
    "        print(f\"\\n=== {model} (HTTP {info['status_code']}) ===\")\n",
    "        print(info[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cead4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ Prompt: T√≥m t·∫Øt ch∆∞∆°ng 1 c·ªßa t√†i li·ªáu PDF cho t√¥i.\n",
      "\n",
      "üéâ Gemini tr·∫£ l·ªùi:\n",
      " <coroutine object call_gemini at 0x00000255D3B40C40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klein\\AppData\\Local\\Temp\\ipykernel_16788\\3923267052.py:4: RuntimeWarning: coroutine 'call_gemini' was never awaited\n",
      "  print(\"\\nüéâ Gemini tr·∫£ l·ªùi:\\n\", call_gemini(test_prompt))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test l·∫°i call_gemini\n",
    "test_prompt = \"T√≥m t·∫Øt ch∆∞∆°ng 1 c·ªßa t√†i li·ªáu PDF cho t√¥i.\"\n",
    "print(\"üëâ Prompt:\", test_prompt)\n",
    "print(\"\\nüéâ Gemini tr·∫£ l·ªùi:\\n\", call_gemini(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daaab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: ƒê·ªãnh nghƒ©a wrapper GeminiLLM d√πng call_gemini\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    api_key:    str\n",
    "    max_output_tokens: int = 1000\n",
    "    temperature:       float = 0.6\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # G·ªçi h√†m call_gemini ƒë√£ ƒë·ªãnh nghƒ©a ·ªü Cell 3\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe93b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m gemini_llm = GeminiLLM(\n\u001b[32m      6\u001b[39m     model_name=MODEL_ID,            \u001b[38;5;66;03m# t·ª´ Cell 2\u001b[39;00m\n\u001b[32m      7\u001b[39m     api_key=API_KEY,                \u001b[38;5;66;03m# t·ª´ Cell 2\u001b[39;00m\n\u001b[32m      8\u001b[39m     max_output_tokens=MAX_TOKENS,   \u001b[38;5;66;03m# t·ª´ Cell 2\u001b[39;00m\n\u001b[32m      9\u001b[39m     temperature=TEMP                \u001b[38;5;66;03m# t·ª´ Cell 2\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2) T·∫°o RetrievalQA chain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m retriever = \u001b[43mvectordb\u001b[49m.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m})\n\u001b[32m     14\u001b[39m qa_chain = RetrievalQA.from_chain_type(\n\u001b[32m     15\u001b[39m     llm=gemini_llm,\n\u001b[32m     16\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     retriever=retriever,\n\u001b[32m     18\u001b[39m     return_source_documents=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 3) ƒê·ªãnh nghƒ©a h√†m ask ƒë·ªÉ h·ªèi v√† in k·∫øt qu·∫£ + ngu·ªìn\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 6: Kh·ªüi t·∫°o pipeline RAG v·ªõi GeminiLLM v√† vectordb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Kh·ªüi t·∫°o GeminiLLM\n",
    "gemini_llm = GeminiLLM(\n",
    "    model_name=MODEL_ID,            # t·ª´ Cell 2\n",
    "    api_key=API_KEY,                # t·ª´ Cell 2\n",
    "    max_output_tokens=MAX_TOKENS,   # t·ª´ Cell 2\n",
    "    temperature=TEMP                # t·ª´ Cell 2\n",
    ")\n",
    "\n",
    "# 2) T·∫°o RetrievalQA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 3) ƒê·ªãnh nghƒ©a h√†m ask ƒë·ªÉ h·ªèi v√† in k·∫øt qu·∫£ + ngu·ªìn\n",
    "def ask(question: str):\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"=== ANSWER ===\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata.get('source', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7081e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ";-; ƒêang th·ª≠ model: gemini-1.5-pro\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "[429] H·∫øt quota ho·∫∑c qu√° t·∫£i ·ªü gemini-1.5-pro. Th·ª≠ l·∫°i sau...\n",
      "\n",
      ";-; ƒêang th·ª≠ model: gemini-2.0\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-2.0:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "[404] Model gemini-2.0 kh√¥ng t·ªìn t·∫°i. B·ªè qua.\n",
      "\n",
      ";-; ƒêang th·ª≠ model: gemini-2.0-flash\n",
      "[DEBUG] G·ªçi URL: https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "=== ANSWER ===\n",
      "Based on the provided texts, here's what can be said about \"Smells and Heuristics\" in general:\n",
      "\n",
      "*   **Definition:** Smells and heuristics are indicators of potential problems in code. They represent observations and rules of thumb used by experienced developers to identify areas that might need improvement or refactoring.\n",
      "*   **Purpose:** They guide developers in writing, reading, and cleaning code. By recognizing smells, developers can make informed decisions about how to improve the code's structure, readability, and maintainability.\n",
      "*   **Origin:** The concept of \"Code Smells\" was popularized by Martin Fowler in his book \"Refactoring.\" The context also mentions the author adding their own smells and heuristics to the list.\n",
      "*   **Cross-References:** The context provides an \"Appendix C\" dedicated to cross-referencing smells and heuristics. This cross-reference maps specific codes (like C1, C2, G1, G2, etc.) to page numbers where those smells/heuristics are discussed in more detail. This is intended to help readers find and understand the context in which each heuristic is applied.\n",
      "*   **Application through Case Studies:** The introduction emphasizes the importance of studying case studies to understand how smells and heuristics\n",
      "\n",
      "=== SOURCES ===\n",
      "- CleanCode.pdf_page_316\n",
      "- CleanCode.pdf_page_440\n",
      "- CleanCode.pdf_page_323\n",
      "- CleanCode.pdf_page_28\n"
     ]
    }
   ],
   "source": [
    "ask(\"describe details as much as you can about Smell and Heuristic General.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
