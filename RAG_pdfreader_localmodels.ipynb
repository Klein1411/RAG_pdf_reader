{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755146bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# --- 1. XoÃ¡ thÆ° má»¥c chromadb cÅ© náº¿u tá»“n táº¡i ---\n",
    "if os.path.isdir(\"chromadb\"):\n",
    "    print(\"Äang xÃ³a thÆ° má»¥c chromadb cÅ©...\")\n",
    "    shutil.rmtree(\"chromadb\")\n",
    "\n",
    "# --- 3. Kiá»ƒm tra vÃ  cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t ---\n",
    "def ensure_package(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Thiáº¿u thÆ° viá»‡n '{pkg_name}', Ä‘ang cÃ i Ä‘áº·t...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        __import__(import_name or pkg_name)\n",
    "\n",
    "pkgs = [\n",
    "    (\"chromadb\", None),\n",
    "    (\"langchain\", None),\n",
    "    (\"ollama\", None),\n",
    "    (\"tiktoken\", None),\n",
    "    (\"PyPDF2\", None)\n",
    "]\n",
    "\n",
    "for pkg, imp in pkgs:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Äá»c PDF vá»›i decoder UTF-8 Ä‘á»ƒ trÃ¡nh lá»—i tuple ---\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "pdf_path = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"  # Thay Ä‘Æ°á»ng dáº«n tá»›i file PDF cá»§a báº¡n\n",
    "reader = PdfReader(pdf_path)\n",
    "docs = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    raw_text = page.extract_text() or \"\"\n",
    "    # Náº¿u raw_text lÃ  bytes, decode báº±ng utf-8 vÃ  bá» kÃ½ tá»± khÃ´ng há»£p lá»‡\n",
    "    if isinstance(raw_text, (bytes, bytearray)):\n",
    "        text = raw_text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = raw_text\n",
    "    docs.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": f\"{os.path.basename(pdf_path)}_page_{i+1}\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sá»­ dá»¥ng embedding model local\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b79c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Táº¡o hoáº·c táº£i láº¡i vector database local vá»›i fallback khi embed tháº¥t báº¡i ---\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"ğŸ‰ VectorDB khá»Ÿi táº¡o thÃ nh cÃ´ng vá»›i model embedding hiá»‡n táº¡i.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"[Warning] Embed tháº¥t báº¡i: {e}\")\n",
    "    print(\"Chuyá»ƒn sang model embedding thay tháº¿: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    # CÃ i vÃ  dÃ¹ng MiniLM embedding local thay tháº¿\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    fallback_embed = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=fallback_embed,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    embed_model = fallback_embed  # cáº­p nháº­t cho pháº§n truy váº¥n sau\n",
    "    print(\"ğŸ‰ VectorDB Ä‘Ã£ khá»Ÿi táº¡o láº¡i vá»›i MiniLM embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : âœ“\n",
      "Model     : gemini-2.5-pro\n",
      "MaxTokens : 1048\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # Ä‘á»c cÃ¡c biáº¿n GEMINI_API_KEY, GEMINI_MODEL,â€¦\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\") # vÃ­ dá»¥: \"gemini-2.5-pro\"\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"Báº¡n lÃ  má»™t trá»£ lÃ½ há»¯u Ã­ch. HÃ£y tráº£ lá»i Ä‘áº§y Ä‘á»§ vÃ  chi tiáº¿t. Äá»«ng láº·p láº¡i cÃ¢u tráº£ lá»i náº¿u khÃ´ng cáº§n thiáº¿t.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Kiá»ƒm tra\n",
    "print(\"API Key   :\", \"âœ“\" if API_KEY else \"âœ—\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Äá»‹nh nghÄ©a vÃ  test vá»›i debug\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from IPython.display import clear_output\n",
    "# Patch Ä‘á»ƒ cháº¡y asyncio.run()/await trong Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\")       # \"gemini-2.5-pro\"\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "BASE_URL = f\"https://generativelanguage.googleapis.com/v1/models/{MODEL_ID}:generateContent\"\n",
    "\n",
    "async def generate_content(prompt: str) -> dict:\n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\n",
    "            \"maxOutputTokens\": MAX_TOKENS,\n",
    "            \"temperature\": TEMP\n",
    "        }\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(BASE_URL, params={\"key\": API_KEY}, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "async def main(prompt: str):\n",
    "    result = await generate_content(prompt)\n",
    "    \n",
    "    # 1) In raw response Ä‘á»ƒ debug\n",
    "    # print(\"ğŸ” Raw response:\")\n",
    "    # import pprint; pprint.pprint(result)\n",
    "    \n",
    "    # 2) Show finishReason náº¿u cÃ³\n",
    "    fr = result.get(\"candidates\", [{}])[0].get(\"finishReason\")\n",
    "    print(f\"\\nâš™ï¸ finishReason: {fr}\")\n",
    "\n",
    "    # 3) Extract text náº¿u cÃ³\n",
    "    cands = result.get(\"candidates\", [])\n",
    "    if not cands:\n",
    "        print(\"âŒ No candidates returned.\")\n",
    "        return\n",
    "    content_obj = cands[0].get(\"content\", {})\n",
    "    parts = content_obj.get(\"parts\") or []\n",
    "    # GhÃ©p text vÃ  loáº¡i bá» newline ngay táº¡i Ä‘Ã¢y:\n",
    "    text = \"\".join(p.get(\"text\", \"\") for p in parts)\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    if text:\n",
    "        print(\"\\nğŸ‰ Gemini tráº£ lá»i Ä‘áº§y Ä‘á»§:\\n\")\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ KhÃ´ng tÃ¬m tháº¥y text trong parts.\")\n",
    "        print(\" Náº¿u finishReason lÃ  MAX_TOKENS, thá»­ tÄƒng GEMINI_MAX_TOKENS lÃªn 512 hoáº·c 1024 vÃ  cháº¡y láº¡i.\")\n",
    "    \n",
    "    \n",
    "    display(Markdown(clean_text))\n",
    "    \n",
    "    # clear_output(wait=True)\n",
    "    # print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33bd0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™ï¸ finishReason: STOP\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Cháº¯c cháº¯n rá»“i! DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i Ä‘oáº¡n vÄƒn giá»›i thiá»‡u vá» cÃ¢y xanh, báº¡n cÃ³ thá»ƒ chá»n Ä‘oáº¡n phÃ¹ há»£p nháº¥t vá»›i má»¥c Ä‘Ã­ch cá»§a mÃ¬nh nhÃ©. --- ### Máº«u 1 (Giá»›i thiá»‡u chung) CÃ¢y xanh lÃ  má»™t pháº§n khÃ´ng thá»ƒ thiáº¿u vÃ  vÃ´ cÃ¹ng ká»³ diá»‡u cá»§a sá»± sá»‘ng trÃªn TrÃ¡i Äáº¥t. Vá»›i bá»™ rá»… cáº¯m sÃ¢u vÃ o lÃ²ng Ä‘áº¥t Ä‘á»ƒ tÃ¬m kiáº¿m nguá»“n dinh dÆ°á»¡ng, thÃ¢n cÃ¢y vÆ°Æ¡n cao máº¡nh máº½, vÃ  tÃ¡n lÃ¡ xum xuÃª Ä‘Ã³n láº¥y Ã¡nh náº¯ng máº·t trá»i, má»—i cÃ¡i cÃ¢y lÃ  má»™t nhÃ  mÃ¡y sinh há»c hoÃ n háº£o. CÃ¢y khÃ´ng chá»‰ cung cáº¥p oxy trong lÃ nh cho chÃºng ta hÃ­t thá»Ÿ, mÃ  cÃ²n lÃ  \"mÃ¡i nhÃ \" cá»§a vÃ´ sá»‘ loÃ i chim vÃ  sinh váº­t khÃ¡c. Tá»« bÃ³ng mÃ¡t dá»‹u hiá»n trong ngÃ y hÃ¨ oi áº£ Ä‘áº¿n nhá»¯ng mÃ¹a quáº£ ngá»t, cÃ¢y xanh tháº§m láº·ng cá»‘ng hiáº¿n vÃ  lÃ m cho hÃ nh tinh cá»§a chÃºng ta trá»Ÿ nÃªn tÆ°Æ¡i Ä‘áº¹p vÃ  Ä‘Ã¡ng sá»‘ng hÆ¡n. --- ### Máº«u 2 (ThiÃªn vá» cáº£m xÃºc vÃ  thÆ¡ má»™ng) Tá»« bao Ä‘á»i nay, cÃ¢y xanh Ä‘Ã£ Ä‘á»©ng Ä‘Ã³ nhÆ° má»™t ngÆ°á»i báº¡n hiá»n hÃ²a, chá»©ng kiáº¿n bao thÄƒng tráº§m cá»§a thá»i gian. CÃ¢y lÃ  biá»ƒu tÆ°á»£ng cá»§a sá»©c sá»‘ng mÃ£nh liá»‡t vÃ  sá»± kiÃªn cÆ°á»ng, dÃ¹ trong sÆ°Æ¡ng giÃ³ bÃ£o bÃ¹ng váº«n vÆ°Æ¡n mÃ¬nh máº¡nh máº½. CÃ¢y cho ta bÃ³ng mÃ¡t Ä‘á»ƒ nghá»‰ ngÆ¡i, cho ta tiáº¿ng lÃ¡ xÃ o xáº¡c nhÆ° má»™t báº£n nháº¡c Ãªm Ä‘á»m cá»§a thiÃªn nhiÃªn, vÃ  cho ta nhá»¯ng sáº¯c mÃ u rá»±c rá»¡ khi chuyá»ƒn mÃ¹a. NhÃ¬n má»™t máº§m non vÆ°Æ¡n lÃªn tá»« lÃ²ng Ä‘áº¥t hay má»™t cÃ¢y cá»• thá»¥ vá»¯ng chÃ£i, ta cáº£m nháº­n Ä‘Æ°á»£c sá»± hÃ o phÃ³ng vÃ  bÃ¬nh yÃªn mÃ  thiÃªn nhiÃªn ban táº·ng. --- ### Máº«u 3 (ÄÆ¡n giáº£n, dÃ nh cho tráº» em) CÃ¢y xanh lÃ  má»™t ngÆ°á»i báº¡n ráº¥t tuyá»‡t vá»i cá»§a chÃºng ta! CÃ¢y cÃ³ rá»… Ä‘á»ƒ uá»‘ng nÆ°á»›c dÆ°á»›i Ä‘áº¥t, cÃ³ thÃ¢n cÃ¢y to khá»e Ä‘á»ƒ Ä‘á»©ng vá»¯ng, vÃ  cÃ³ ráº¥t nhiá»u cÃ nh lÃ¡ Ä‘á»ƒ Ä‘Ã³n náº¯ng. Nhá» cÃ³ cÃ¢y xanh, chÃºng ta cÃ³ khÃ´ng khÃ­ trong lÃ nh Ä‘á»ƒ thá»Ÿ, cÃ³ bÃ³ng mÃ¡t Ä‘á»ƒ vui chÆ¡i, vÃ  cÃ²n cÃ³ cáº£ nhá»¯ng loáº¡i quáº£ ngon Æ¡i lÃ  ngon ná»¯a. CÃ¡c báº¡n chim cÅ©ng ráº¥t thÃ­ch lÃ m tá»• trÃªn cÃ¢y. VÃ¬ váº­y, chÃºng ta hÃ£y cÃ¹ng nhau yÃªu thÆ°Æ¡ng vÃ  báº£o vá»‡ nhá»¯ng ngÆ°á»i báº¡n cÃ¢y xanh nhÃ©"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cháº¡y test\n",
    "test_prompt = \"HÃ£y viáº¿t má»™t Ä‘oáº¡n vÄƒn nhá» giá»›i thiá»‡u vá» cÃ¢y.\"\n",
    "asyncio.run(main(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Äá»‹nh nghÄ©a wrapper GeminiLLM dÃ¹ng call_gemini\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    api_key:    str\n",
    "    max_output_tokens: int = 1000\n",
    "    temperature:       float = 0.6\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Gá»i hÃ m call_gemini Ä‘Ã£ Ä‘á»‹nh nghÄ©a á»Ÿ Cell 3\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Khá»Ÿi táº¡o pipeline RAG vá»›i GeminiLLM vÃ  vectordb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Khá»Ÿi táº¡o GeminiLLM\n",
    "gemini_llm = GeminiLLM(\n",
    "    model_name=MODEL_ID,            # tá»« Cell 2\n",
    "    api_key=API_KEY,                # tá»« Cell 2\n",
    "    max_output_tokens=MAX_TOKENS,   # tá»« Cell 2\n",
    "    temperature=TEMP                # tá»« Cell 2\n",
    ")\n",
    "\n",
    "# 2) Táº¡o RetrievalQA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 3) Äá»‹nh nghÄ©a hÃ m ask Ä‘á»ƒ há»i vÃ  in káº¿t quáº£ + nguá»“n\n",
    "def ask(question: str):\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"=== ANSWER ===\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata.get('source', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7081e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"describe details as much as you can about Smell and Heuristic General.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
