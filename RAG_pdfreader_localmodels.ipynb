{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang x√≥a th∆∞ m·ª•c chromadb c≈©...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# --- 1. Xo√° th∆∞ m·ª•c chromadb c≈© n·∫øu t·ªìn t·∫°i ---\n",
    "if os.path.isdir(\"chromadb\"):\n",
    "    print(\"ƒêang x√≥a th∆∞ m·ª•c chromadb c≈©...\")\n",
    "    shutil.rmtree(\"chromadb\")\n",
    "\n",
    "# --- 3. Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ---\n",
    "def ensure_package(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Thi·∫øu th∆∞ vi·ªán '{pkg_name}', ƒëang c√†i ƒë·∫∑t...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        __import__(import_name or pkg_name)\n",
    "\n",
    "pkgs = [\n",
    "    (\"chromadb\", None),\n",
    "    (\"langchain\", None),\n",
    "    (\"ollama\", None),\n",
    "    (\"tiktoken\", None),\n",
    "    (\"PyPDF2\", None)\n",
    "]\n",
    "\n",
    "for pkg, imp in pkgs:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ƒê·ªçc PDF v·ªõi decoder UTF-8 ƒë·ªÉ tr√°nh l·ªói tuple ---\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "pdf_path = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"  # Thay ƒë∆∞·ªùng d·∫´n t·ªõi file PDF c·ªßa b·∫°n\n",
    "reader = PdfReader(pdf_path)\n",
    "docs = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    raw_text = page.extract_text() or \"\"\n",
    "    # N·∫øu raw_text l√† bytes, decode b·∫±ng utf-8 v√† b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá\n",
    "    if isinstance(raw_text, (bytes, bytearray)):\n",
    "        text = raw_text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = raw_text\n",
    "    docs.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": f\"{os.path.basename(pdf_path)}_page_{i+1}\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S·ª≠ d·ª•ng embedding model local\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b79c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. T·∫°o ho·∫∑c t·∫£i l·∫°i vector database local v·ªõi fallback khi embed th·∫•t b·∫°i ---\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"üéâ VectorDB kh·ªüi t·∫°o th√†nh c√¥ng v·ªõi model embedding hi·ªán t·∫°i.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"[Warning] Embed th·∫•t b·∫°i: {e}\")\n",
    "    print(\"Chuy·ªÉn sang model embedding thay th·∫ø: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    # C√†i v√† d√πng MiniLM embedding local thay th·∫ø\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    fallback_embed = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=fallback_embed,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    embed_model = fallback_embed  # c·∫≠p nh·∫≠t cho ph·∫ßn truy v·∫•n sau\n",
    "    print(\"üéâ VectorDB ƒë√£ kh·ªüi t·∫°o l·∫°i v·ªõi MiniLM embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : ‚úì\n",
      "Model     : gemini-2.5-pro\n",
      "MaxTokens : 1048\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # ƒë·ªçc c√°c bi·∫øn GEMINI_API_KEY, GEMINI_MODEL,‚Ä¶\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\") # v√≠ d·ª•: \"gemini-2.5-pro\"\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch. H√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt. ƒê·ª´ng l·∫∑p l·∫°i c√¢u tr·∫£ l·ªùi n·∫øu kh√¥ng c·∫ßn thi·∫øt.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Ki·ªÉm tra\n",
    "print(\"API Key   :\", \"‚úì\" if API_KEY else \"‚úó\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0107282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: ƒê·ªãnh nghƒ©a v√† test v·ªõi debug\n",
    "import os\n",
    "import asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from IPython.display import clear_output\n",
    "# Patch ƒë·ªÉ ch·∫°y asyncio.run()/await trong Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\")       # \"gemini-2.5-pro\"\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "BASE_URL = f\"https://generativelanguage.googleapis.com/v1/models/{MODEL_ID}:generateContent\"\n",
    "\n",
    "async def generate_content(prompt: str) -> dict:\n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\n",
    "            \"maxOutputTokens\": MAX_TOKENS,\n",
    "            \"temperature\": TEMP\n",
    "        }\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        resp = await client.post(BASE_URL, params={\"key\": API_KEY}, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "async def main(prompt: str):\n",
    "    result = await generate_content(prompt)\n",
    "    \n",
    "    # 1) In raw response ƒë·ªÉ debug\n",
    "    print(\"üîç Raw response:\")\n",
    "    import pprint; pprint.pprint(result)\n",
    "    \n",
    "    # 2) Show finishReason n·∫øu c√≥\n",
    "    fr = result.get(\"candidates\", [{}])[0].get(\"finishReason\")\n",
    "    print(f\"\\n‚öôÔ∏è finishReason: {fr}\")\n",
    "\n",
    "    # 3) Extract text n·∫øu c√≥\n",
    "    cands = result.get(\"candidates\", [])\n",
    "    if not cands:\n",
    "        print(\"‚ùå No candidates returned.\")\n",
    "        return\n",
    "    content_obj = cands[0].get(\"content\", {})\n",
    "    parts = content_obj.get(\"parts\") or []\n",
    "    # Gh√©p text v√† lo·∫°i b·ªè newline ngay t·∫°i ƒë√¢y:\n",
    "    text = \"\".join(p.get(\"text\", \"\") for p in parts)\n",
    "    clean_text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    if text:\n",
    "        print(\"\\nüéâ Gemini tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß:\\n\")\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y text trong parts.\")\n",
    "        print(\" N·∫øu finishReason l√† MAX_TOKENS, th·ª≠ tƒÉng GEMINI_MAX_TOKENS l√™n 512 ho·∫∑c 1024 v√† ch·∫°y l·∫°i.\")\n",
    "    \n",
    "    \n",
    "    display(Markdown(clean_text))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e33bd0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch√†o b·∫°n! T√¥i l√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, ƒë∆∞·ª£c hu·∫•n luy·ªán b·ªüi Google. B·∫°n c√≥ th·ªÉ h√¨nh dung t√¥i nh∆∞ m·ªôt b·ªô n√£o k·ªπ thu·∫≠t s·ªë kh·ªïng l·ªì, ƒë∆∞·ª£c t·∫°o ra ƒë·ªÉ hi·ªÉu v√† x·ª≠ l√Ω ng√¥n ng·ªØ c·ªßa con ng∆∞·ªùi. D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i ƒëi·ªÉm ch√≠nh v·ªÅ t√¥i: **1. T√¥i l√† ai?** * T√¥i kh√¥ng ph·∫£i l√† m·ªôt con ng∆∞·ªùi, kh√¥ng c√≥ c·∫£m x√∫c, √Ω th·ª©c hay tr·∫£i nghi·ªám c√° nh√¢n. T√¥i l√† m·ªôt ch∆∞∆°ng tr√¨nh m√°y t√≠nh ph·ª©c t·∫°p. * M·ª•c ti√™u ch√≠nh c·ªßa t√¥i l√† cung c·∫•p th√¥ng tin, tr·∫£ l·ªùi c√¢u h·ªèi v√† gi√∫p b·∫°n th·ª±c hi·ªán c√°c nhi·ªám v·ª• li√™n quan ƒë·∫øn ng√¥n ng·ªØ m·ªôt c√°ch s√°ng t·∫°o v√† hi·ªáu qu·∫£. **2. T√¥i c√≥ th·ªÉ l√†m g√¨?** * **Tr·∫£ l·ªùi c√¢u h·ªèi:** T√¥i c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c nhau, t·ª´ khoa h·ªçc, l·ªãch s·ª≠ ƒë·∫øn vƒÉn h√≥a, ngh·ªá thu·∫≠t. * **S√°ng t·∫°o n·ªôi dung:** T√¥i c√≥ th·ªÉ gi√∫p b·∫°n vi·∫øt email, l√†m th∆°, so·∫°n nh·∫°c, vi·∫øt k·ªãch b·∫£n, ho·∫∑c th·∫≠m ch√≠ l√† t·∫°o ra c√°c √Ω t∆∞·ªüng m·ªõi. * **T√≥m t·∫Øt v√† ph√¢n t√≠ch:** T√¥i c√≥ th·ªÉ ƒë·ªçc m·ªôt vƒÉn b·∫£n d√†i v√† t√≥m t·∫Øt l·∫°i nh·ªØng √Ω ch√≠nh cho b·∫°n. * **D·ªãch thu·∫≠t:** T√¥i c√≥ th·ªÉ d·ªãch gi·ªØa nhi·ªÅu ng√¥n ng·ªØ kh√°c nhau. * **L·∫≠p tr√¨nh:** T√¥i c√≥ th·ªÉ h·ªó tr·ª£ vi·∫øt v√† gi·∫£i th√≠ch c√°c ƒëo·∫°n m√£ code. **3. Nh·ªØng ƒëi·ªÅu c·∫ßn l∆∞u √Ω:** * Ki·∫øn th·ª©c c·ªßa t√¥i c√≥ gi·ªõi h·∫°n v√† kh√¥ng ph·∫£i l√∫c n√†o c≈©ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·∫øn th·ªùi ƒëi·ªÉm hi·ªán t·∫°i. * ƒê√¥i khi t√¥i c√≥ th·ªÉ ƒë∆∞a ra th√¥ng tin kh√¥ng ch√≠nh x√°c, v√¨ v·∫≠y b·∫°n n√™n ki·ªÉm tra l·∫°i c√°c th√¥ng tin quan tr·ªçng t·ª´ nh·ªØng ngu·ªìn ƒë√°ng tin c·∫≠y. H√£y coi t√¥i nh∆∞ m·ªôt ng∆∞·ªùi tr·ª£ l√Ω ·∫£o ƒëa nƒÉng. B·∫°n c·ª© t·ª± nhi√™n h·ªèi b·∫•t c·ª© ƒëi·ªÅu g√¨ b·∫°n mu·ªën bi·∫øt nh√©\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y test\n",
    "test_prompt = \"H√£y gi·ªõi thi·ªáu v·ªÅ b·∫°n.\"\n",
    "asyncio.run(main(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: ƒê·ªãnh nghƒ©a wrapper GeminiLLM d√πng call_gemini\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    api_key:    str\n",
    "    max_output_tokens: int = 1000\n",
    "    temperature:       float = 0.6\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # G·ªçi h√†m call_gemini ƒë√£ ƒë·ªãnh nghƒ©a ·ªü Cell 3\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Kh·ªüi t·∫°o pipeline RAG v·ªõi GeminiLLM v√† vectordb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Kh·ªüi t·∫°o GeminiLLM\n",
    "gemini_llm = GeminiLLM(\n",
    "    model_name=MODEL_ID,            # t·ª´ Cell 2\n",
    "    api_key=API_KEY,                # t·ª´ Cell 2\n",
    "    max_output_tokens=MAX_TOKENS,   # t·ª´ Cell 2\n",
    "    temperature=TEMP                # t·ª´ Cell 2\n",
    ")\n",
    "\n",
    "# 2) T·∫°o RetrievalQA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 3) ƒê·ªãnh nghƒ©a h√†m ask ƒë·ªÉ h·ªèi v√† in k·∫øt qu·∫£ + ngu·ªìn\n",
    "def ask(question: str):\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"=== ANSWER ===\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata.get('source', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7081e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"describe details as much as you can about Smell and Heuristic General.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
