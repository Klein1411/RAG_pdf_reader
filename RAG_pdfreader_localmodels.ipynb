{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755146bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xóa thư mục chromadb cũ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# --- 1. Xoá thư mục chromadb cũ nếu tồn tại ---\n",
    "if os.path.isdir(\"chromadb\"):\n",
    "    print(\"Đang xóa thư mục chromadb cũ...\")\n",
    "    shutil.rmtree(\"chromadb\")\n",
    "\n",
    "# --- 3. Kiểm tra và cài đặt các thư viện cần thiết ---\n",
    "def ensure_package(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        print(f\"Thiếu thư viện '{pkg_name}', đang cài đặt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        __import__(import_name or pkg_name)\n",
    "\n",
    "pkgs = [\n",
    "    (\"chromadb\", None),\n",
    "    (\"langchain\", None),\n",
    "    (\"ollama\", None),\n",
    "    (\"tiktoken\", None),\n",
    "    (\"PyPDF2\", None)\n",
    "]\n",
    "\n",
    "for pkg, imp in pkgs:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d7fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Đọc PDF với decoder UTF-8 để tránh lỗi tuple ---\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "pdf_path = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"  # Thay đường dẫn tới file PDF của bạn\n",
    "reader = PdfReader(pdf_path)\n",
    "docs = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    raw_text = page.extract_text() or \"\"\n",
    "    # Nếu raw_text là bytes, decode bằng utf-8 và bỏ ký tự không hợp lệ\n",
    "    if isinstance(raw_text, (bytes, bytearray)):\n",
    "        text = raw_text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        text = raw_text\n",
    "    docs.append(Document(\n",
    "        page_content=text,\n",
    "        metadata={\"source\": f\"{os.path.basename(pdf_path)}_page_{i+1}\"}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2051d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79dc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klein\\AppData\\Local\\Temp\\ipykernel_43292\\1977429318.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng embedding model local\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b79c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 VectorDB khởi tạo thành công với model embedding hiện tại.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klein\\AppData\\Local\\Temp\\ipykernel_43292\\4150500916.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Tạo hoặc tải lại vector database local với fallback khi embed thất bại ---\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"🎉 VectorDB khởi tạo thành công với model embedding hiện tại.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"[Warning] Embed thất bại: {e}\")\n",
    "    print(\"Chuyển sang model embedding thay thế: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    # Cài và dùng MiniLM embedding local thay thế\n",
    "    from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "    fallback_embed = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=fallback_embed,\n",
    "        persist_directory=\"chromadb\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    embed_model = fallback_embed  # cập nhật cho phần truy vấn sau\n",
    "    print(\"🎉 VectorDB đã khởi tạo lại với MiniLM embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73e3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : ✓\n",
      "Model     : gemini-2.0-flash\n",
      "MaxTokens : 256\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # đọc các biến GEMINI_API_KEY, GEMINI_MODEL,…\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash\")\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"Bạn là một trợ lý hữu ích. Hãy trả lời đầy đủ và chi tiết. Đừng lặp lại câu trả lời nếu không cần thiết.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Kiểm tra\n",
    "print(\"API Key   :\", \"✓\" if API_KEY else \"✗\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399de756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Định nghĩa hàm call_gemini(prompt) – đảm bảo luôn return str\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"Please set GEMINI_API_KEY in .env\")\n",
    "\n",
    "    is_chat = MODEL_ID.lower().startswith(\"chat-\")\n",
    "    # Build endpoint + payload\n",
    "    if is_chat:\n",
    "        version = \"v1beta\"\n",
    "        endpoint = \":generateMessage\"\n",
    "        body_key = \"messages\"\n",
    "        body_val = [{\"author\": \"user\", \"content\": prompt}]\n",
    "        payload = {\n",
    "            body_key: body_val,\n",
    "            \"generationConfig\": {\"maxOutputTokens\": MAX_TOKENS, \"temperature\": TEMP}\n",
    "        }\n",
    "    else:\n",
    "        if MODEL_ID.endswith(\"-001\"):\n",
    "            version = \"v1\"\n",
    "            endpoint = \":generateText\"\n",
    "            body_key = \"prompt\"\n",
    "            body_val = {\"text\": prompt}\n",
    "            payload = {\n",
    "                body_key: body_val,\n",
    "                \"maxOutputTokens\": MAX_TOKENS,\n",
    "                \"temperature\": TEMP\n",
    "            }\n",
    "        else:\n",
    "            version = \"v1beta\"\n",
    "            endpoint = \":generateContent\"\n",
    "            body_key = \"contents\"\n",
    "            body_val = [{\"parts\": [{\"text\": prompt}]}]\n",
    "            payload = {\n",
    "                body_key: body_val,\n",
    "                \"generationConfig\": {\"maxOutputTokens\": MAX_TOKENS, \"temperature\": TEMP}\n",
    "            }\n",
    "\n",
    "    url = f\"https://generativelanguage.googleapis.com/{version}/models/{MODEL_ID}{endpoint}?key={API_KEY}\"\n",
    "    resp = requests.post(url, json=payload, headers={\"Content-Type\":\"application/json\"}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # Lấy candidate đầu tiên\n",
    "    cand = data.get(\"candidates\", [{}])[0]\n",
    "\n",
    "    # Chat case: unwrap message.content.parts -> str\n",
    "    if is_chat:\n",
    "        msg = cand.get(\"message\", {}).get(\"content\", {})\n",
    "        # msg có thể là string hoặc dict với parts\n",
    "        if isinstance(msg, str):\n",
    "            return msg\n",
    "        parts = msg.get(\"parts\", [])\n",
    "        return \"\".join([p.get(\"text\", \"\") for p in parts])\n",
    "\n",
    "    # Text-only case: output or content\n",
    "    out = cand.get(\"output\", \"\") or cand.get(\"content\", \"\")\n",
    "    if isinstance(out, str):\n",
    "        return out\n",
    "    # Nếu out là dict (e.g. {parts:[...]})\n",
    "    parts = out.get(\"parts\", [])\n",
    "    if parts:\n",
    "        return \"\".join([p.get(\"text\", \"\") for p in parts])\n",
    "    # fallback\n",
    "    return str(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cead4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👉 Prompt: Tóm tắt chương 1 của tài liệu PDF cho tôi.\n",
      "\n",
      "🎉 Gemini trả lời:\n",
      " Để tôi tóm tắt chương 1 của tài liệu PDF cho bạn, vui lòng cung cấp tài liệu PDF đó. Bạn có thể làm điều này bằng cách:\n",
      "\n",
      "*   **Tải lên tệp PDF trực tiếp cho tôi.** (Nếu nền tảng bạn đang sử dụng cho phép)\n",
      "*   **Cung cấp liên kết đến tệp PDF.** (Nếu nó có sẵn trực tuyến)\n",
      "*   **Sao chép và dán nội dung của chương 1 vào khung chat.** (Nếu chương đó không quá dài)\n",
      "\n",
      "Sau khi bạn cung cấp thông tin này, tôi sẽ đọc và tóm tắt chương 1 cho bạn.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test lại call_gemini\n",
    "test_prompt = \"Tóm tắt chương 1 của tài liệu PDF cho tôi.\"\n",
    "print(\"👉 Prompt:\", test_prompt)\n",
    "print(\"\\n🎉 Gemini trả lời:\\n\", call_gemini(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daaab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Định nghĩa wrapper GeminiLLM dùng call_gemini\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM, BaseModel):\n",
    "    model_name: str\n",
    "    api_key:    str\n",
    "    max_output_tokens: int = 1000\n",
    "    temperature:       float = 0.6\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Gọi hàm call_gemini đã định nghĩa ở Cell 3\n",
    "        return call_gemini(prompt)\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe93b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Khởi tạo pipeline RAG với GeminiLLM và vectordb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Khởi tạo GeminiLLM\n",
    "gemini_llm = GeminiLLM(\n",
    "    model_name=MODEL_ID,            # từ Cell 2\n",
    "    api_key=API_KEY,                # từ Cell 2\n",
    "    max_output_tokens=MAX_TOKENS,   # từ Cell 2\n",
    "    temperature=TEMP                # từ Cell 2\n",
    ")\n",
    "\n",
    "# 2) Tạo RetrievalQA chain\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 3) Định nghĩa hàm ask để hỏi và in kết quả + nguồn\n",
    "def ask(question: str):\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"=== ANSWER ===\")\n",
    "    print(result[\"result\"])\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata.get('source', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7081e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER ===\n",
      "Based on the provided context, here's what I can tell you about \"Smells and Heuristics\" in general:\n",
      "\n",
      "*   **Definition:** Smells and heuristics are indicators of potential problems in code. They represent observations and guidelines that experienced developers use to identify areas that might need refactoring or further attention.\n",
      "\n",
      "*   **Origin:** The concept of \"Code Smells\" was popularized by Martin Fowler in his book \"Refactoring.\" The text provided expands on Fowler's ideas and includes additional smells and heuristics.\n",
      "\n",
      "*   **Purpose:** Smells and heuristics are intended to guide developers in writing, reading, and cleaning code. They help developers understand their reactions to code and provide a basis for making informed decisions about code quality and maintainability.\n",
      "\n",
      "*   **Application:** The text emphasizes the importance of applying heuristics in the context of real-world code changes. The case studies in the book demonstrate how heuristics are used to make specific decisions during code cleanup. Cross-references are provided to link heuristics to the specific changes made in the case studies.\n",
      "\n",
      "*   **Examples:** The provided text lists several specific smells and heuristics, including:\n",
      "\n",
      "    *   **G9: Dead Code:** Code that is not executed, such as code within an `\n",
      "\n",
      "=== SOURCES ===\n",
      "- CleanCode.pdf_page_316\n",
      "- CleanCode.pdf_page_440\n",
      "- CleanCode.pdf_page_323\n",
      "- CleanCode.pdf_page_28\n"
     ]
    }
   ],
   "source": [
    "ask(\"describe details as much as you can about Smell and Heuristic General.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
