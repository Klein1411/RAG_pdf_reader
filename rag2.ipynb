{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dadc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: d:\\Project_self\n",
      "Will remove: d:\\Project_self\\chromadb_v3\n",
      "Exists? True\n",
      "Removed: True\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "persist_dir = \"./chromadb_v3\"\n",
    "abs_dir = os.path.abspath(persist_dir)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"Will remove:\", abs_dir)\n",
    "print(\"Exists?\", os.path.exists(abs_dir))\n",
    "\n",
    "if os.path.exists(abs_dir):\n",
    "    # Xóa thư mục cũ (bỏ qua lỗi nếu có)\n",
    "    shutil.rmtree(abs_dir, ignore_errors=True)\n",
    "    print(\"Removed:\", not os.path.exists(abs_dir))\n",
    "else:\n",
    "    print(\"No directory to remove.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c135ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: d:\\Project_self\n",
      "Will remove: d:\\Project_self\\chromadb_v3\n",
      "Exists? False\n"
     ]
    }
   ],
   "source": [
    "# check đường dẫn đúng để xóa db hỗ trợ code trên, để mỗi lần đọc 1 file pdf mới thì nó sẽ reset\n",
    "import os\n",
    "persist_dir = \"./chromadb_v3\"\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"Will remove:\", os.path.abspath(persist_dir))\n",
    "print(\"Exists?\", os.path.exists(persist_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b10a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store đã sẵn sàng tại ./chromadb_v3\n",
      "T8 ....................................................................................16-275, 17-335 T9 ............................................................................................... …\n",
      "G28 ..................................................................................15-262, 17-317 G29 ..................................................................................15-262, 17-31 …\n",
      "331             case MARCH:   332             case APRIL:   333             case MAY:   334             case JUNE:   335             case JULY:   336             case AUGUST:   337             case SE …\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # ép thành bytes rồi decode utf-8, bỏ ký tự lạ\n",
    "    return s.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def build_chroma_from_single_pdf(\n",
    "    pdf_path: str,\n",
    "    persist_dir: str = \"./chromadb_v3\",\n",
    "    chunk_size: int = 1500,\n",
    "    chunk_overlap: int = 200,\n",
    "    hf_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device: str | None = None,\n",
    "    batch_size: int = 64\n",
    ") -> Chroma:\n",
    "    # 1) Xóa cache cũ\n",
    "    if os.path.isdir(persist_dir):\n",
    "        shutil.rmtree(persist_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "    # 2) Load PDF\n",
    "    assert os.path.isfile(pdf_path), f\"File không tồn tại: {pdf_path}\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # 3) Normalize text trên từng document\n",
    "    for d in docs:\n",
    "        d.page_content = normalize_text(d.page_content)\n",
    "\n",
    "    # 4) Split thành chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # 5) Embedding\n",
    "    model_kwargs = {\"device\": device} if device else {}\n",
    "    hf_emb = HuggingFaceEmbeddings(\n",
    "        model_name=hf_model,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs={\"batch_size\": batch_size}\n",
    "    )\n",
    "\n",
    "    # 6) Build Chroma store\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=hf_emb,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Vector store đã sẵn sàng tại {persist_dir}\")\n",
    "    return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = r\"D:\\Project_self\\pdf_place\\CleanCode.pdf\"\n",
    "    vectordb = build_chroma_from_single_pdf(\n",
    "        pdf_path    = pdf_file,\n",
    "        persist_dir = \"./chromadb_v3\",\n",
    "        device      = None   # hoặc \"cuda\"\n",
    "    )\n",
    "\n",
    "    # Thử truy vấn\n",
    "    for doc in vectordb.similarity_search(\"Your query here\", k=3):\n",
    "        print(doc.page_content[:200].replace(\"\\n\",\" \"), \"…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7561d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fee38",
   "metadata": {},
   "source": [
    "\\C:\\Users\\klein\\AppData\\Local\\nomic.ai\\GPT4All\\Llama-3.2-1B-Instruct-Q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9092828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpt4all import GPT4All\n",
    "\n",
    "# # 1. Khởi tạo model với đúng keyword args\n",
    "# gpt = GPT4All(\n",
    "#     model_name     = \"Llama-3.2-1B-Instruct-Q4_0.gguf\",       # TÊN file\n",
    "#     model_path     = \"C:/Users/klein/AppData/Local/nomic.ai/GPT4All\",  # THƯ MỤC CHỨA file\n",
    "#     model_type     = \"llama\",                                  # định dạng\n",
    "#     allow_download = False,\n",
    "#     n_threads      = 4,\n",
    "#     device         = \"cpu\",\n",
    "#     verbose        = True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d548697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found model file at 'C:\\\\Users\\\\klein\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\Llama-3.2-1B-Instruct-Q4_0.gguf'\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "gpt = GPT4All(\n",
    "    model_name     = \"Llama-3.2-1B-Instruct-Q4_0.gguf\",\n",
    "    model_path     = \"C:/Users/klein/AppData/Local/nomic.ai/GPT4All\",\n",
    "    model_type     = \"llama\",\n",
    "    allow_download = False,\n",
    "    n_threads      = 4,            # vẫn cho CPU đa luồng nếu cần\n",
    "    device         = \"cuda\",       # bật GPU\n",
    "    verbose        = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5bb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE:  Xin hãy cho tôi biết một số thông tin về bản thân.\n",
      "\n",
      "Tôi xin lỗi vì trước đó đã không trả lời câu hỏi của bạn. Tôi là một người trẻ tuổi đang theo đuổi sự nghiệp trong lĩnh vực công nghệ và phát triển ứng dụng di động. Tôi thích viết code, học cách sử dụng các nền\n",
      "STREAMING:\n",
      " Xin hãy cho tôi biết thêm về bản thân.\n",
      "\n",
      "Tôi xin lỗi, nhưng tôi không thể cung cấp thông tin cá nhân. Tôi chỉ có thể giúp\n"
     ]
    }
   ],
   "source": [
    "# 2. Sinh văn bản không streaming\n",
    "#    Kết quả trả về có thể là list hoặc str tuỳ version, in luôn kết quả\n",
    "resp = gpt.generate(\n",
    "    \"Xin chào, bạn tên là gì?\",   # prompt\n",
    "    max_tokens = 64,              # số token sinh tối đa\n",
    "    temp       = 0.7,             # nhiệt độ\n",
    "    top_k      = 40,\n",
    "    top_p      = 0.4\n",
    ")\n",
    "print(\"GENERATE:\", resp)\n",
    "\n",
    "# 3. Hoặc sinh có streaming (xuất token từng bước)\n",
    "print(\"STREAMING:\")\n",
    "for tok in gpt.generate(\n",
    "    \"Xin chào, bạn tên là gì?\",\n",
    "    max_tokens =30,\n",
    "    temp       = 0.7,\n",
    "    streaming  = True\n",
    "):\n",
    "    print(tok, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72abcc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(model_name: 'str', *, model_path: 'str | os.PathLike[str] | None' = None, model_type: 'str | None' = None, allow_download: 'bool' = True, n_threads: 'int | None' = None, device: 'str | None' = None, n_ctx: 'int' = 2048, ngl: 'int' = 100, verbose: 'bool' = False)\n",
      "(self, prompt: 'str', *, max_tokens: 'int' = 200, temp: 'float' = 0.7, top_k: 'int' = 40, top_p: 'float' = 0.4, min_p: 'float' = 0.0, repeat_penalty: 'float' = 1.18, repeat_last_n: 'int' = 64, n_batch: 'int' = 8, n_predict: 'int | None' = None, streaming: 'bool' = False, callback: 'ResponseCallbackType' = <function empty_response_callback at 0x0000027A188F6DE0>) -> 'Any'\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "print(inspect.signature(GPT4All))\n",
    "print(inspect.signature(GPT4All.generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77cc37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready.\n"
     ]
    }
   ],
   "source": [
    "hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2) Load lại Chroma store đã persist ở folder chromadb_offline\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"./chromadb_offline\",    # folder bạn đã lưu vector store\n",
    "    embedding_function=hf_emb\n",
    ")\n",
    "\n",
    "# 3) Tạo retriever, lấy top k chunks\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Retriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7218268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Tạo hàm RAG tay, trả về cả answer và docs\n",
    "def rag_query(question: str) -> tuple[str, list]:\n",
    "    # 3a) Lấy các đoạn liên quan\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 3b) Gom hết nội dung ngữ cảnh vào biến context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"(page {d.metadata.get('page', '?')}): {d.page_content.strip()}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "    # 3c) Xây prompt cho GPT4All\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. You also need to translate language when user need. Do NOT repeat yourself—provide one concise final report.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"When you finish, output the line: ### END ###\\n\"\n",
    "        \"Answer:\"\n",
    ")\n",
    "\n",
    "    # 3d) Gọi GPT4All và nhận về answer\n",
    "    resp = gpt.generate(\n",
    "        prompt,\n",
    "        max_tokens=1000,\n",
    "        temp=0.7,\n",
    "        top_k=40,\n",
    "        top_p=0.4,\n",
    "        streaming=False\n",
    "    )\n",
    "    # Nếu trả về list, lấy phần tử đầu\n",
    "    answer = resp[0] if isinstance(resp, list) else resp\n",
    "\n",
    "    # Trả về cả answer và list docs\n",
    "    return answer, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821d81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo chain RetrievalQA\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True\n",
    "# )\n",
    "\n",
    "# print(\"QA chain ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043d3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER ===\n",
      " \n",
      "\n",
      "### The principle of clean code\n",
      "\n",
      "The principle of clean code refers to a set of guidelines that aim to reduce the amount of unnecessary or redundant code in software. This is achieved by applying design patterns, writing clear and concise code, and following best practices for coding.\n",
      "\n",
      "To apply this principle to incoming AI trends:\n",
      "\n",
      "1.  **Separate Concerns**: Break down complex problems into smaller, manageable parts.\n",
      "2.  **Use Design Patterns**: Apply established patterns such as the Singleton or Factory pattern to reduce duplication of effort.\n",
      "3.  **Write Clear and Concise Code**: Use clear variable names, concise comments, and avoid unnecessary complexity.\n",
      "\n",
      "### END ###\n",
      "\n",
      "Note: The provided answer is a single response that addresses both questions about \"The principle of clean code\" and its application in AI trends. It follows the specified format to provide one final report with a line at the end (\"### END ###\").\n",
      "\n",
      "=== SOURCES ===\n"
     ]
    }
   ],
   "source": [
    "# 6) Thử truy vấn\n",
    "question = \"What is the priciple of Clean code and how can it be apply to incoming ai trend\"  # sửa câu hỏi\n",
    "answer, docs = rag_query(question)   # unpack cả answer lẫn docs\n",
    "\n",
    "print(\"=== ANSWER ===\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n=== SOURCES ===\")\n",
    "seen_pages = set()\n",
    "for src in docs:\n",
    "    page = src.metadata.get(\"page\", \"unknown\")\n",
    "    # chỉ in mỗi page một lần\n",
    "    if page in seen_pages:\n",
    "        continue\n",
    "    seen_pages.add(page)\n",
    "    snippet = src.page_content.replace(\"\\n\", \" \").strip()[:200]\n",
    "    print(f\"- (page {page}) {snippet}…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
