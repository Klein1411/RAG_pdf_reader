{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda0dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key   : ✓\n",
      "Model     : gemini-1.5-pro\n",
      "MaxTokens : 256\n",
      "Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, requests\n",
    "\n",
    "load_dotenv()  # đọc các biến GEMINI_API_KEY, GEMINI_MODEL,…\n",
    "\n",
    "API_KEY    = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_ID   = os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-pro\")\n",
    "PROMPT     = os.getenv(\n",
    "    \"GEMINI_PROMPT\",\n",
    "    \"Bạn là một trợ lý hữu ích. Hãy trả lời đầy đủ và chi tiết. Đừng lặp lại câu trả lời nếu không cần thiết.\"\n",
    ")\n",
    "MAX_TOKENS = int(os.getenv(\"GEMINI_MAX_TOKENS\", \"256\"))\n",
    "TEMP       = float(os.getenv(\"GEMINI_TEMPERATURE\", \"0.7\"))\n",
    "\n",
    "# Kiểm tra\n",
    "print(\"API Key   :\", \"✓\" if API_KEY else \"✗\")\n",
    "print(\"Model     :\", MODEL_ID)\n",
    "print(\"MaxTokens :\", MAX_TOKENS)\n",
    "print(\"Temperature:\", TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc91fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "last_call_time = 0\n",
    "\n",
    "# Danh sách model ưu tiên\n",
    "MODEL_FALLBACKS = [\n",
    "    \"gemini-1.5-pro\",\n",
    "    \"gemini-2.0\",\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-1.5-flash\"\n",
    "]\n",
    "\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    global last_call_time\n",
    "\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"Please set GEMINI_API_KEY in .env\")\n",
    "\n",
    "    for model in MODEL_FALLBACKS:\n",
    "        now = time.time()\n",
    "        # Giới hạn tốc độ: tối thiểu 0.5 giây giữa 2 lần gọi\n",
    "        if now - last_call_time < 0.5:\n",
    "            time.sleep(0.5 - (now - last_call_time))\n",
    "        last_call_time = time.time()\n",
    "\n",
    "        print(f\"\\n;-; Đang thử model: {model}\")\n",
    "\n",
    "        # Xác định version\n",
    "        if model.startswith(\"gemini-2.0\") or model.startswith(\"gemini-1.5\"):\n",
    "            version = \"v1\"\n",
    "        else:\n",
    "            version = \"v1beta\"  # cho exp cũ (nếu thêm sau)\n",
    "        endpoint = \":generateContent\"\n",
    "\n",
    "        payload = {\n",
    "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\n",
    "                \"maxOutputTokens\": MAX_TOKENS,\n",
    "                \"temperature\": TEMP\n",
    "            }\n",
    "        }\n",
    "\n",
    "        url = f\"https://generativelanguage.googleapis.com/{version}/models/{model}{endpoint}?key={API_KEY}\"\n",
    "        print(f\"[DEBUG] Gọi URL: {url}\")\n",
    "\n",
    "        # --- Retry loop ---\n",
    "        for attempt in range(3):\n",
    "            resp = requests.post(\n",
    "                url,\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            if resp.status_code == 429:\n",
    "                print(f\"[429] Hết quota hoặc quá tải ở {model}. Thử lại sau...\")\n",
    "                break  # thoát retry → thử model kế tiếp\n",
    "\n",
    "            if resp.status_code == 404:\n",
    "                print(f\"[404] Model {model} không tồn tại. Bỏ qua.\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "                cand = data.get(\"candidates\", [{}])[0]\n",
    "                out = cand.get(\"output\", \"\") or cand.get(\"content\", \"\")\n",
    "                if isinstance(out, str):\n",
    "                    return out\n",
    "                parts = out.get(\"parts\", [])\n",
    "                return \"\".join([p.get(\"text\", \"\") for p in parts])\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Model {model} gặp lỗi: {e}\")\n",
    "                break\n",
    "\n",
    "    raise RuntimeError(\"❌ Tất cả model đều hết quota hoặc lỗi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257ff5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-.- Prompt: Tóm tắt chương 1 của tài liệu PDF cho tôi.\n",
      "\n",
      ";-; Đang thử model: gemini-1.5-pro\n",
      "[DEBUG] Gọi URL: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "[429] Hết quota hoặc quá tải ở gemini-1.5-pro. Thử lại sau...\n",
      "\n",
      ";-; Đang thử model: gemini-2.0\n",
      "[DEBUG] Gọi URL: https://generativelanguage.googleapis.com/v1/models/gemini-2.0:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "[404] Model gemini-2.0 không tồn tại. Bỏ qua.\n",
      "\n",
      ";-; Đang thử model: gemini-2.0-flash\n",
      "[DEBUG] Gọi URL: https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent?key=AIzaSyDYR-CMmT8QYLH2eMvxoUfRzN03Vp-uQhA\n",
      "\n",
      "`v` Gemini trả lời:\n",
      " Để tôi tóm tắt chương 1 của tài liệu PDF cho bạn, tôi cần bạn cung cấp cho tôi nội dung của chương đó. Bạn có thể:\n",
      "\n",
      "*   **Sao chép và dán nội dung chương 1 vào đây.**\n",
      "*   **Tải tài liệu PDF lên một dịch vụ chia sẻ tệp và cung cấp liên kết cho tôi.**\n",
      "\n",
      "Khi tôi có nội dung của chương 1, tôi sẽ đọc và cung cấp cho bạn một bản tóm tắt ngắn gọn.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test lại call_gemini\n",
    "test_prompt = \"Tóm tắt chương 1 của tài liệu PDF cho tôi.\"\n",
    "print(\"-.- Prompt:\", test_prompt)\n",
    "print(\"\\n`v` Gemini trả lời:\\n\", call_gemini(test_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
