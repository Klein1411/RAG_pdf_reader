# %%
import os
import sys
import subprocess
import shutil
import io

# --- 1. Xo√° th∆∞ m·ª•c chromadb c≈© n·∫øu t·ªìn t·∫°i ---
if os.path.isdir("chromadb"):
    print("ƒêang x√≥a th∆∞ m·ª•c chromadb c≈©...")
    shutil.rmtree("chromadb")

# --- 3. Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ---
def ensure_package(pkg_name, import_name=None):
    try:
        __import__(import_name or pkg_name)
    except ImportError:
        print(f"Thi·∫øu th∆∞ vi·ªán '{pkg_name}', ƒëang c√†i ƒë·∫∑t...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg_name])
        __import__(import_name or pkg_name)

pkgs = [
    ("chromadb", None),
    ("langchain", None),
    ("ollama", None),
    ("tiktoken", None),
    ("PyPDF2", None)
]

for pkg, imp in pkgs:
    ensure_package(pkg, imp)

# %%
# --- ƒê·ªçc PDF v·ªõi decoder UTF-8 ƒë·ªÉ tr√°nh l·ªói tuple ---
import os
from PyPDF2 import PdfReader
from langchain.schema import Document

pdf_path = r"D:\Project_self\pdf_place\CleanCode.pdf"  # Thay ƒë∆∞·ªùng d·∫´n t·ªõi file PDF c·ªßa b·∫°n
reader = PdfReader(pdf_path)
docs = []

for i, page in enumerate(reader.pages):
    raw_text = page.extract_text() or ""
    # N·∫øu raw_text l√† bytes, decode b·∫±ng utf-8 v√† b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá
    if isinstance(raw_text, (bytes, bytearray)):
        text = raw_text.decode("utf-8", errors="ignore")
    else:
        text = raw_text
    docs.append(Document(
        page_content=text,
        metadata={"source": f"{os.path.basename(pdf_path)}_page_{i+1}"}
    ))

# %%
from PyPDF2 import PdfReader
from langchain.schema import Document
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# %%
# S·ª≠ d·ª•ng embedding model local
embed_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# %%
# --- 7. T·∫°o ho·∫∑c t·∫£i l·∫°i vector database local v·ªõi fallback khi embed th·∫•t b·∫°i ---
try:
    vectordb = Chroma.from_documents(
        documents=docs,
        embedding=embed_model,
        persist_directory="chromadb"
    )
    vectordb.persist()
    print("üéâ VectorDB kh·ªüi t·∫°o th√†nh c√¥ng v·ªõi model embedding hi·ªán t·∫°i.")
    
except ValueError as e:
    print(f"[Warning] Embed th·∫•t b·∫°i: {e}")
    print("Chuy·ªÉn sang model embedding thay th·∫ø: sentence-transformers/all-MiniLM-L6-v2")
    # C√†i v√† d√πng MiniLM embedding local thay th·∫ø
    from langchain.embeddings import SentenceTransformerEmbeddings
    fallback_embed = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = Chroma.from_documents(
        documents=docs,
        embedding=fallback_embed,
        persist_directory="chromadb"
    )
    vectordb.persist()
    embed_model = fallback_embed  # c·∫≠p nh·∫≠t cho ph·∫ßn truy v·∫•n sau
    print("üéâ VectorDB ƒë√£ kh·ªüi t·∫°o l·∫°i v·ªõi MiniLM embedding.")

# %%
from dotenv import load_dotenv
import os, requests

# ‚úÖ ƒë·ªçc .env ngay trong th∆∞ m·ª•c project (m·∫∑c ƒë·ªãnh)
load_dotenv()

API_KEY    = os.getenv("OPENROUTER_API_KEY")
MODEL_ID   = os.getenv("OPENROUTER_MODEL", "anthropic/claude-3.5-sonnet")
PROMPT     = os.getenv(
    "OPENROUTER_PROMPT",
    "B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch. H√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt. ƒê·ª´ng l·∫∑p l·∫°i c√¢u tr·∫£ l·ªùi n·∫øu kh√¥ng c·∫ßn thi·∫øt."
)
MAX_TOKENS = int(os.getenv("OPENROUTER_MAX_TOKENS", "512"))
TEMP       = float(os.getenv("OPENROUTER_TEMPERATURE", "0.7"))

# Ki·ªÉm tra
print("API Key   :", "‚úì" if API_KEY else "‚úó")
print("Model     :", MODEL_ID)
print("MaxTokens :", MAX_TOKENS)
print("Temperature:", TEMP)



# %%
# Cell 3: ƒê·ªãnh nghƒ©a h√†m call_gemini(prompt) ‚Äì ƒë·∫£m b·∫£o lu√¥n return str
def call_openrouter(prompt: str) -> str:
    global last_call_time

    if not API_KEY:
        raise ValueError("‚ùå Thi·∫øu OPENROUTER_API_KEY trong .env")

    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY.strip()}",
        "Content-Type": "application/json",
        "X-Title": "MyRAGApp"
    }
    payload = {
        "model": "meta-llama/llama-3.1-405b-instruct",   # üëà thay b·∫±ng model m·∫°nh
        "messages": [
            {"role": "system", "content": PROMPT},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": MAX_TOKENS,
        "temperature": TEMP,
    }

    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    if resp.status_code != 200:
        print("‚ùå HTTP", resp.status_code)
        print("Resp error:", resp.text)
        resp.raise_for_status()

    data = resp.json()
    return data["choices"][0]["message"]["content"]



# %%
# Cell 4: Test l·∫°i call_gemini
# test_prompt = "T√≥m t·∫Øt ch∆∞∆°ng 1 c·ªßa t√†i li·ªáu PDF cho t√¥i."
# print("üëâ Prompt:", test_prompt)
# print("\nüéâ Gemini tr·∫£ l·ªùi:\n", call_gemini(test_prompt))

# %%
# Cell 5: ƒê·ªãnh nghƒ©a wrapper GeminiLLM d√πng call_gemini
from typing import List, Optional
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.outputs import ChatResult, ChatGeneration
import requests

class OpenRouterLLM(BaseChatModel):
    model: str
    api_key: str
    max_tokens: int = 512
    temperature: float = 0.7
    url: str = "https://openrouter.ai/api/v1/chat/completions"

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager=None,
        **kwargs
    ) -> ChatResult:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": m.content}
                for m in messages
                if isinstance(m, HumanMessage)
            ],
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "temperature": kwargs.get("temperature", self.temperature),
        }

        resp = requests.post(self.url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]

        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=content))]
        )

    @property
    def _llm_type(self) -> str:
        return "openrouter_custom"


# %%
# Cell 6: Kh·ªüi t·∫°o pipeline RAG v·ªõi GeminiLLM v√† vectordb
from langchain.chains import RetrievalQA

# 1) Kh·ªüi t·∫°o GeminiLLM
llm = OpenRouterLLM(
    model="meta-llama/llama-3.1-405b-instruct",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    max_tokens=512,
    temperature=0.7,
)
retriever = vectordb.as_retriever(search_kwargs={"k": 4})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 3) ƒê·ªãnh nghƒ©a h√†m ask ƒë·ªÉ h·ªèi v√† in k·∫øt qu·∫£ + ngu·ªìn
def ask(question: str):
    result = qa_chain.invoke({"query": question})
    print("=== ANSWER ===")
    print(result["result"])
    print("\n=== SOURCES ===")
    for doc in result["source_documents"]:
        print(f"- {doc.metadata.get('source', '')}")


# %% test run
# ask("describe details as much as you can about Smell and Heuristic General.")


