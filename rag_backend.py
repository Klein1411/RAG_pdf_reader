# %%
import os
import sys
import subprocess
import shutil
import io

# --- 1. XoÃ¡ thÆ° má»¥c chromadb cÅ© náº¿u tá»“n táº¡i ---
if os.path.isdir("chromadb"):
    print("Äang xÃ³a thÆ° má»¥c chromadb cÅ©...")
    shutil.rmtree("chromadb")

# --- 3. Kiá»ƒm tra vÃ  cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t ---
def ensure_package(pkg_name, import_name=None):
    try:
        __import__(import_name or pkg_name)
    except ImportError:
        print(f"Thiáº¿u thÆ° viá»‡n '{pkg_name}', Ä‘ang cÃ i Ä‘áº·t...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg_name])
        __import__(import_name or pkg_name)

pkgs = [
    ("chromadb", None),
    ("langchain", None),
    ("ollama", None),
    ("tiktoken", None),
    ("PyPDF2", None)
]

for pkg, imp in pkgs:
    ensure_package(pkg, imp)

# %%
# --- Äá»c PDF vá»›i decoder UTF-8 Ä‘á»ƒ trÃ¡nh lá»—i tuple ---
import os
from PyPDF2 import PdfReader
from langchain.schema import Document

pdf_path = r"D:\Project_self\pdf_place\CleanCode.pdf"  # Thay Ä‘Æ°á»ng dáº«n tá»›i file PDF cá»§a báº¡n
reader = PdfReader(pdf_path)
docs = []

for i, page in enumerate(reader.pages):
    raw_text = page.extract_text() or ""
    # Náº¿u raw_text lÃ  bytes, decode báº±ng utf-8 vÃ  bá» kÃ½ tá»± khÃ´ng há»£p lá»‡
    if isinstance(raw_text, (bytes, bytearray)):
        text = raw_text.decode("utf-8", errors="ignore")
    else:
        text = raw_text
    docs.append(Document(
        page_content=text,
        metadata={"source": f"{os.path.basename(pdf_path)}_page_{i+1}"}
    ))

# %%
from PyPDF2 import PdfReader
from langchain.schema import Document
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# %%
# Sá»­ dá»¥ng embedding model local
embed_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# %%
# --- 7. Táº¡o hoáº·c táº£i láº¡i vector database local vá»›i fallback khi embed tháº¥t báº¡i ---
try:
    vectordb = Chroma.from_documents(
        documents=docs,
        embedding=embed_model,
        persist_directory="chromadb"
    )
    vectordb.persist()
    print("ğŸ‰ VectorDB khá»Ÿi táº¡o thÃ nh cÃ´ng vá»›i model embedding hiá»‡n táº¡i.")
    
except ValueError as e:
    print(f"[Warning] Embed tháº¥t báº¡i: {e}")
    print("Chuyá»ƒn sang model embedding thay tháº¿: sentence-transformers/all-MiniLM-L6-v2")
    # CÃ i vÃ  dÃ¹ng MiniLM embedding local thay tháº¿
    from langchain.embeddings import SentenceTransformerEmbeddings
    fallback_embed = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = Chroma.from_documents(
        documents=docs,
        embedding=fallback_embed,
        persist_directory="chromadb"
    )
    vectordb.persist()
    embed_model = fallback_embed  # cáº­p nháº­t cho pháº§n truy váº¥n sau
    print("ğŸ‰ VectorDB Ä‘Ã£ khá»Ÿi táº¡o láº¡i vá»›i MiniLM embedding.")

# %%
from dotenv import load_dotenv
import os, requests

load_dotenv()  # Ä‘á»c cÃ¡c biáº¿n GEMINI_API_KEY, GEMINI_MODEL,â€¦

API_KEY    = os.getenv("GEMINI_API_KEY")
MODEL_ID   = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
PROMPT     = os.getenv(
    "GEMINI_PROMPT",
    "Báº¡n lÃ  má»™t trá»£ lÃ½ há»¯u Ã­ch. HÃ£y tráº£ lá»i Ä‘áº§y Ä‘á»§ vÃ  chi tiáº¿t. Äá»«ng láº·p láº¡i cÃ¢u tráº£ lá»i náº¿u khÃ´ng cáº§n thiáº¿t."
)
MAX_TOKENS = int(os.getenv("GEMINI_MAX_TOKENS", "256"))
TEMP       = float(os.getenv("GEMINI_TEMPERATURE", "0.7"))

# Kiá»ƒm tra
print("API Key   :", "âœ“" if API_KEY else "âœ—")
print("Model     :", MODEL_ID)
print("MaxTokens :", MAX_TOKENS)
print("Temperature:", TEMP)



# %%
# Cell 3: Äá»‹nh nghÄ©a hÃ m call_gemini(prompt) â€“ Ä‘áº£m báº£o luÃ´n return str
def call_gemini(prompt: str) -> str:
    if not API_KEY:
        raise ValueError("Please set GEMINI_API_KEY in .env")

    is_chat = MODEL_ID.lower().startswith("chat-")
    # Build endpoint + payload
    if is_chat:
        version = "v1beta"
        endpoint = ":generateMessage"
        body_key = "messages"
        body_val = [{"author": "user", "content": prompt}]
        payload = {
            body_key: body_val,
            "generationConfig": {"maxOutputTokens": MAX_TOKENS, "temperature": TEMP}
        }
    else:
        if MODEL_ID.endswith("-001"):
            version = "v1"
            endpoint = ":generateText"
            body_key = "prompt"
            body_val = {"text": prompt}
            payload = {
                body_key: body_val,
                "maxOutputTokens": MAX_TOKENS,
                "temperature": TEMP
            }
        else:
            version = "v1beta"
            endpoint = ":generateContent"
            body_key = "contents"
            body_val = [{"parts": [{"text": prompt}]}]
            payload = {
                body_key: body_val,
                "generationConfig": {"maxOutputTokens": MAX_TOKENS, "temperature": TEMP}
            }

    url = f"https://generativelanguage.googleapis.com/{version}/models/{MODEL_ID}{endpoint}?key={API_KEY}"
    resp = requests.post(url, json=payload, headers={"Content-Type":"application/json"}, timeout=30)
    resp.raise_for_status()
    data = resp.json()

    # Láº¥y candidate Ä‘áº§u tiÃªn
    cand = data.get("candidates", [{}])[0]

    # Chat case: unwrap message.content.parts -> str
    if is_chat:
        msg = cand.get("message", {}).get("content", {})
        # msg cÃ³ thá»ƒ lÃ  string hoáº·c dict vá»›i parts
        if isinstance(msg, str):
            return msg
        parts = msg.get("parts", [])
        return "".join([p.get("text", "") for p in parts])

    # Text-only case: output or content
    out = cand.get("output", "") or cand.get("content", "")
    if isinstance(out, str):
        return out
    # Náº¿u out lÃ  dict (e.g. {parts:[...]})
    parts = out.get("parts", [])
    if parts:
        return "".join([p.get("text", "") for p in parts])
    # fallback
    return str(out)


# %%
# Cell 4: Test láº¡i call_gemini
# test_prompt = "TÃ³m táº¯t chÆ°Æ¡ng 1 cá»§a tÃ i liá»‡u PDF cho tÃ´i."
# print("ğŸ‘‰ Prompt:", test_prompt)
# print("\nğŸ‰ Gemini tráº£ lá»i:\n", call_gemini(test_prompt))

# %%
# Cell 5: Äá»‹nh nghÄ©a wrapper GeminiLLM dÃ¹ng call_gemini
from langchain.llms.base import LLM
from pydantic import BaseModel
from typing import Optional, List

class GeminiLLM(LLM, BaseModel):
    model_name: str
    api_key:    str
    max_output_tokens: int = 1000
    temperature:       float = 0.6

    @property
    def _llm_type(self) -> str:
        return "gemini"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        # Gá»i hÃ m call_gemini Ä‘Ã£ Ä‘á»‹nh nghÄ©a á»Ÿ Cell 3
        return call_gemini(prompt)

    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        return self._call(prompt, stop)

# %%
# Cell 6: Khá»Ÿi táº¡o pipeline RAG vá»›i GeminiLLM vÃ  vectordb
from langchain.chains import RetrievalQA

# 1) Khá»Ÿi táº¡o GeminiLLM
gemini_llm = GeminiLLM(
    model_name=MODEL_ID,            # tá»« Cell 2
    api_key=API_KEY,                # tá»« Cell 2
    max_output_tokens=MAX_TOKENS,   # tá»« Cell 2
    temperature=TEMP                # tá»« Cell 2
)

# 2) Táº¡o RetrievalQA chain
retriever = vectordb.as_retriever(search_kwargs={"k": 4})
qa_chain = RetrievalQA.from_chain_type(
    llm=gemini_llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 3) Äá»‹nh nghÄ©a hÃ m ask Ä‘á»ƒ há»i vÃ  in káº¿t quáº£ + nguá»“n
def ask(question: str):
    result = qa_chain.invoke({"query": question})
    print("=== ANSWER ===")
    print(result["result"])
    print("\n=== SOURCES ===")
    for doc in result["source_documents"]:
        print(f"- {doc.metadata.get('source', '')}")


# %% test run
# ask("describe details as much as you can about Smell and Heuristic General.")


